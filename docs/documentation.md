## Documentation

The Postgres Performance Farm is a system in charge of running benchmarking tests through different Postgres versions. 

Each benchmark aims to run on a newly installed Postgres, without major changes in the default configurations; however, it is possible to change basic parameters. The install does not depend nor interfere with previously existing installations: all directories and sockets are separate by default.

The currently available test is PgBench, also configurable. Furthermore, the script collects basic system information, such as hardware and operating system. There is also execution of collectd, if available.

##### Terminology

The main assumption of the script is that the most relevant results are collecting running tests through different commits within the Postgres repository. Having the same test ran multiple times without any changes in hardware or version does not provide useful information.

Commonly used words are:

* A **run** is an execution of the script on a single client;
* A **benchmark** is a single execution of PgBench;
* A given run can have multiple benchmarks done;
* A run should have at least one benchmark.

##### Output

The client produces the following output:

* JSON results with output of PgBench and system information;
* Logs per-interval generated by PgBench, with intervals of 1 second;
* Error and operations logs.



### Client script

The client script is written in Python 3 and takes care of all the client-side part, i. e. downloading and installing Postgres, starting a cluster, executing tests, collecting system information and then shutting down processes and deleting unnecessary folders.

The workflow can be summarized as:

1. Creating (or recreating) all necessary directories;
2. Checking for existing Postgres clones within the Performance Farm folders, and if so, trying to pull for updates;
3. Saving current branch and commit;
4. If there has been an update, or a clone from scratch, Postgres is built and reinstalled using `make`;
5. User-defined parameters are being checked to see whether there are errors;
6. A cluster is initialized and started through `pg_ctl`;
7. System information is collected;
8. PgBench is executed:
   1. For each client, initdb is performed;
   2. Tests are run;
9. Cluster is stopped;
10. Data directory is cleaned;
11. Logs are being attached to results and shipped to the server;
12. The script then restarts at step 1 with a different branch, until branches of interests are terminated.



##### PgBench parameters:

* Clients (number of threads);
* Scale of the database;
* Duration of each execution of PgBench;
* Iterations, number of executions. 

Furthermore, PgBench is ran with additional parameters, such as display of statement latencies and logging of interval every second.



##### Files and folders

* perffarm-client.py: takes care of coordinating the setup of benchmarks, and when everything is initialized runs collectors;
* folders.py: global variables related to default folder structure;
* path.py: initialisation of folders for each branch;
* branches.py: list of all branches to use for running tests;
* Benchmarks:
  * pgbench.py: takes care of all PgBench related tasks, such as initialization, run of benchmarks and collecting results;
  * runner.py: wrapper calling PgBench functions and saving output in a file
* Collectors:
  * collectd.py: runs collectd to gather system and database statistics;
  * collector.py: combines other collectors and calls them;
  * system.py: contains a collection of Python3 functions from external modules to extract system information such as CPU usage, kernel configuration and memory;
  * postgres.py: mainly a function that connects to Postgres and selects its settings;
* Post example (removed in later versions): 
  * upload.py: takes the output file and sends it to the API;
* Utils: 
  * cluster.py: initalizes, starts and stops a Postgres cluster;
  * build.py: module which takes care of executing a build from source from a git repository;
  * locking.py: ensures locking of files;
  * logging.py: prints nice logging;
  * misc.py: connects to database and returns available RAM.

##### Tuning settings

The Performance Farm comes with some default settings as well as local settings that can be adjusted. Parameters that can be changed are:

* UPDATE, flag to instruct the script to try to pull for new commits at every execution;
* AUTOMATIC_UPLOAD, flag to enable uploading to the server (works only with a valid machine secret);
* GIT_URL, used to specify the repository to pull from;
* BASE_PATH, folder which will be used for Performance Farm files;
* API_URL, current URL of the server (can be changed for local developing);
* MACHINE_SECRET, identifier of the machine on which benchmarks are being run (generated by the server);
* POSTGRES_CONFIG, basic Postgres parameters;
* DATABASE_NAME, database name;
* PGBENCH_CONFIG, parameters to tune PgBench.

##### Folder structure

All folders used within the Performance Farm are children of BASE_PATH, specified in the settings. Specifically, there are:

* BUILD_PATH, containing files generated by configure;
* INSTALL_PATH, installation directory of make;
* BIN_PATH, bin directory of the Postgres installation;
* OUTPUT_PATH, containing all output files generated by PgBench and the script;
* REPOSITORY_PATH, clone of the Postgres remote in which updates are checked;
* DATADIR_PATH, Postgres data directory, getting removed after every execution;
* SOCKET_PATH, folder for sockets to avoid interfering with other Postgres processes;
* LOG_PATH, containing all logs, errors and messages generated by the script.

##### Interpreting results

Output of the script is saved in a JSON format to take advantage of key-value storage, useful for collecting and parsed.

Results are contained in the output folder. There are two main output files: results.json, containing actual results, and results_complete.json, which is the file getting shipped to the server and is equal to the results file plus all the logs attached in sequence. It lacks human readability, therefore the file which should be checked is the first. 

Main fields of the JSON output are:

* pgbench, with a number of nested objects corresponding to the number of runs, each of them with basic information about latency and tps;

* Linux information, divided in:

  * CPU data, along with system times;
  * OS information, its version and architecture;
  * Memory, containing virtual, swap, mounts;
  * Disk usage with I/O;
  * Process information;
  * Compilers (make and gcc);
  * Collectd results;
  * Postgres configuraton;
  * Meta information (date, time, user name).

  

### API

The API is written in Django and Django-REST, using versions compatible with the Postgres website. Its purposes are:

* Defining a database structure for results;
* Receiving, parsing and inserting data;
* Calculating intermediate and aggregate values;
* Checking for correctness and completeness when database constraints are not sufficient;
* Exposing endpoints to interact with the database.

##### Database structure

The database is being generated by Django and defined through models. Each folder corresponds to a category, and each model file contains several table definitions related to the same object. 

Benchmarks: 

* PgBenchBenchmark, containing configuration for each specific PgBench benchmark, which has to be unique:
  * *pgbench_benchmark_id*;
  * *clients*, number of clients;
  * *scale*, scaling of the database;
  * *duration*, duration of each single PgBench execution;
  * *read_only*, boolean to define whether the test is read-only or read-write.
* PgBenchResult, containing for each run its respective results for each iteration of PgBench:
  * *pgbench_result_id*;
  * *run_id*, foreign key of the run;
  * *benchmark_config*, foreign key containing the identifier of the configuration;
  * *tps*: TPS of the single benchmark;
  * *mode*: mode of the single benchmark (simple);
  * *latency*: latency of the single benchmark;
  * *start*: start time of the single benchmark;
  * *end*: end time of the single benchmark;
  * *iteration*: number of iteration among the total iterations for the specific configuration;
  * *init*: init time of the single benchmark;
* PgBenchRunStatement, containing the statements (parameter -r of PgBench) for each execution:
  * *pgbench_run_statement_id*;
  * *result_id*, foreign key from PgBenchResult;
  * *line_id*, line of the SQL command;
  * *latency*, latency of the test;
  * *result_id*, foreign key for PgBenchStatement to access the description of each statement.
* PgBenchStatement, storing the description of each statement:
  * *pgbench_statement_id*;
  * *statement*, text description.
* PgBenchLog, storing the log for per-interval summary data:
  * *pgbench_log_id*;
  * *pgbench_result_id*;
  * *interval_start*: start of the interval;
  * *num_transactions*: number of transactions within the interval;
  * *sum_latency*: sum of the transaction latencies within the interval;
  * *sum_latency_2*: sum of squares of the transaction latencies within the interval;
  * *min_latency*: minimum latency;
  * *max_latency*: maximum latency.

Machines: 

* Machine, table storing all information related to single machines:
  * *machine_id*;
  * *add_time*, time of the machine being added;
  * *alias*, unique textual string giving an human readable identifier;
  * *description*, editable text description;
  * *machine_secret*, unique string used to pair each machine with each run of tests,only known to machine owner;
  * *approved*, flag identifying whether the machine is coming from a known user, set by admins;
  * *owner_id*, foreign key from Users;
  * *machine_type*, distinguishing the type of the kernel (fixed, added when the first result is stored);

Postgres:

* PostgresSettingSet, table with hash representation of each set of Postgres configurations (since files are big, it is easy to hash main parameters to check whether the same configuration exists):
  * *postgres_settings_set_id*;
  * *settings_sha256*, hash string.
* PostgresSettings, containing all textual information related to Postgres configuration logs corresponding to each hash string (each line represents a single parameter):
  * *postgres_settings_id*;
  * *db_settings_id*, foreign key from PostgresSettingsSet;
  * *setting_name*, name of the parameter;
  * *setting_id*, id of the parameter;
  * *setting_value*, actual value of the parameter.

Runs:

* GitRepo, table containing all information about known Git repositories:
  * *git_repo_id*;
  * *url*: url of the repository;
  * *owner*: responsible of the repository;
* Branch, table with known branches and user-defined ones:
  * *branch_id*;
  * *name*: branch alias;
  * *default_view*, boolean to identify branches of interest;
  * *ordering*: crescent ordering in the website;
  * *git_repo*: corresponding repository.

* RunInfo, table corresponding to information for each run (each execution of the script):
  * *run_id*;
  * *machine_id*, foreign key from Machines;
  * *add_time*, time when the server received the test result;
  * *os_version_id*, foreign key with OS version information;
  * *os_kernel_version_id*, foreign key with kernel information;
  * *hardware_info*, foreign key containing set of specific information related to the operating system;
  * *sysctl_raw*: JSON field with raw output from sysctl;
  * *git_commit*: hash of the commit;
  * *run_received_time*, time when the client received the command of starting a run;
  * *run_start_time*, time when the run is actually executed (might be delayed);
  * *run_end_time*, end time of the run;
  * *git_clone_log*;
  * *git_clone_runtime*;
  * *git_pull_runtime*;
  * *git_repo*: foreign key with git repository;
  * *configure_log*, related to the configure operation while installing Postgres;
  * *configure_runtime*;
  * *build_log*, related to make;
  * *build_runtime*;
  * *install_log*, related to make install;
  * *install_runtime*;
  * *cleanup_log*, time spent removing the data directory folder;
  * *cleanup_runtime*;
  * *benchmark_log*: PgBench log output text;
  * *benchmark*: name of the benchmark (PgBench by default);
  * *postgres_log*, log of all Postgres related operations (pg_ctl);
  * *postgres_info*, output of pg_settings table and foreign key from PostgresSettingsSet.

Systems:

* HardwareInfo, table storing all information related to hardware and its physical characteristics: 
  * *hardware_info_id*;
  * *cpu_brand*;
  * *hz*;
  * *cpu_cores*;
  * *total_memory*, JSON field with all memory information;
  * *total_swap*, JSON field with swap data;
  * *mounts*, JSON field with mounts list;
  * *mounts_hash*, string to verify uniqueness of mounts.
* Compilers, table with all detected compilers to build and install Postgres:
  * *compiler_id*;
  * *compiler*: compiler name.
* OsDistributor, table with all distributors of operating systems:
  * *os_distributor_id*;
  * *dist_name*, distributor name.
* Kernel, table with all known kernels:
  * *kernel_id*;
  * *kernel_name*: unique kernel name (Darwin, Linux).
* OsKernelVersion, table with known kernels along with their version to store updates:
  * *os_kernel_version_id*;
  * *kernel_id*: foreign key identifying the kernel;
  * *kernel_release*: current release;
  * *kernel_version*: current version.
* OsVersion, table with updates of the operating system:
  * *os_version_id*;
  * *dist_id*: foreign key from the distributors table;
  * *description*: string containing the OS version;
  * *release*: current release;
  * *codename*: commonly known codename of the release (Buster, Bionic Beaver).

* Users:
  * Users, Django base model with predefined features such as username, password and email.

In addition to those tables, there is a table dedicated to storing all logs of insertions which failed at some point; if even insertion to the log table fails, the error gets saved to a text file on the server.

All transactions are atomic: if uploading or storing fails at some point, nothing is saved from the result in the database.

##### Community authentication

Authentication currently integrates the existing system on the Postgres infrastructure: the authentication module has been taken from the official pgweb repository, and all related endpoints are also tied to that. 

Currently, the two APIs and the website communicate through cookies, which get saved in the browser and passed in every request which requires authentication. 

The API also implements basic admin features, however at least the first admin user needs to be created through command line on the hosting server. Admins have CRUD privileges on all data (in progress).



### Website

The website is written using Vue.js, using version 2.6.10, and takes advantage of a few of its dedicated packages to handle style, cached data and routing.

This section will be updated in the next days with detailed explanations of the structure and how to contribute, since the website is still in development - stay tuned!